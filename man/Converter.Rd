% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Converter.R
\name{Converter}
\alias{Converter}
\title{Converter of an artificial Neural Network}
\description{
This class analyzes a passed Neural Network and stores its internal
structure and the individual layers by converting the entire network into a
\code{\link[torch]{nn_module}}. With the help of this converter, many
methods of interpretable machine learning are provided, which give a better
understanding of the whole model or individual predictions.
You can use models from the following libraries:
\itemize{
\item \code{\link[keras]{keras}},
\item \code{\link[neuralnet]{neuralnet}}
}

Furthermore, a model can be passed as a list (see details for more
information).
}
\details{
In order to better understand and analyze the prediction of a neural
network, the preactivation or other information of the individual layers,
which are not stored in an ordinary forward pass, are often required. For
this reason, a given neural network is converted into a torch-based neural
network, which provides all the necessary information for an interpretation.
The converted torch model is stored in the field \code{model} and is an instance
of \code{\link[innsight:ConvertedModel]{innsight::ConvertedModel}}.
But before the torch model is created, all relevant details of the passed
model are extracted in a named list stored in the field \code{model_dict}. This
named list has the form as described in the next section.
\subsection{Implemented libraries}{

The converter is implemented for models from the libraries
\code{\link[neuralnet]{neuralnet}} and \code{\link[keras]{keras}}. But you
can also write a wrapper for other libraries because a model can be passed
as a named list with the following components:
\itemize{
\item \strong{\verb{$input_dim}}\cr
An integer vector with the model input dimension, e.g. for
a dense layer with 5 input features use \code{c(5)} or for  a 1d-convolutional
layer with signal length 50 and 4 channels use \code{c(4,50)}.
\item \strong{\verb{$input_names}}\cr
A list with the names for each input dimension, e.g. for
a dense layer with 3 input features use \code{list(c("X1", "X2", "X3"))} or for a
1d-convolutional layer with signal length 5 and 2 channels use
\code{list(c("C1", "C2"), c("L1","L2","L3","L4","L5"))}.
\item \strong{\verb{$output_dim}}\cr
An integer vector with the model output dimension
analogous to \verb{$input_dim}.
\item \strong{\verb{$output_names}}\cr
A list with the names for each output dimension
analogous to \verb{$input_names}.
\item \strong{\verb{$layers}}\cr
A list with the respective layers of the model. Each layer is represented as
another list that requires the following entries depending on the type:
\itemize{
\item \strong{Dense Layer:}
\itemize{
\item \strong{\verb{$type}}: \code{'Dense'}
\item \strong{\verb{$weight}}: The weight matrix of the dense layer with shape
(\code{dim_out}, \code{dim_in}).
\item \strong{\verb{$bias}}: The bias vector of the dense layer with length
\code{dim_out}.
\item \strong{\code{activation_name}}: The name of the activation function for this
dense layer, e.g. \code{'relu'}, \code{'tanh'} or \code{'softmax'}.
\item \strong{\code{dim_in}}: The input dimension of this layer. This value is not
necessary, but helpful to check the format of the weight matrix.
\item \strong{\code{dim_out}}: The output dimension of this layer. This value is not
necessary, but helpful to check the format of the weight matrix.
}
\item \strong{Convolutional Layers:}
\itemize{
\item \strong{\verb{$type}}: \code{'Conv1D'} or \code{'Conv2D'}
\item \strong{\verb{$weight}}: The weight array of the convolutional layer with shape
(\code{out_channels}, \code{in_channels}, \code{kernel_length}) for 1d or
(\code{out_channels}, \code{in_channels}, \code{kernel_height}, \code{kernel_width}) for
2d.
\item \strong{\verb{$bias}}: The bias vector of the layer with length \code{out_channels}.
\item \strong{\verb{$activation_name}}: The name of the activation function for this
layer, e.g. \code{'relu'}, \code{'tanh'} or \code{'softmax'}.
\item \strong{\verb{$dim_in}}: The input dimension of this layer according to the
format (\code{in_channels}, \code{in_length}) for 1d or
(\code{in_channels}, \code{in_height}, \code{in_width}) for 2d.
\item \strong{\verb{$dim_out}}: The output dimension of this layer according to the
format (\code{out_channels}, \code{out_length}) for 1d or
(\code{out_channels}, \code{out_height}, \code{out_width}) for 2d.
\item \strong{\verb{$stride}}: The stride of the convolution (single integer for 1d
and tuple of two integers for 2d). If this value is not specified, the
default values (1d: \code{1} and 2d: \code{c(1,1)}) are used.
\item \strong{\verb{$padding}}: Zero-padding added to the sides of the input before
convolution. For 1d-convolution a tuple of the form
(\code{pad_left}, \code{pad_right}) and for 2d-convolution
(\code{pad_left}, \code{pad_right}, \code{pad_top}, \code{pad_bottom}) is required. If this
value is not specified, the default values (1d: \code{c(0,0)} and 2d:
\code{c(0,0,0,0)}) are used.
\item \strong{\verb{$dilation}}: Spacing between kernel elements (single integer for
1d and tuple of two integers for 2d). If this value is not specified,
the default values (1d: \code{1} and 2d: \code{c(1,1)}) are used.
}
}
\item \strong{Flatten Layer:}
\itemize{
\item \strong{\verb{$dim_in} :} The input dimension of this layer without the batch
dimension.
\item \strong{\verb{$dim_out} :} The output dimension of this layer without the batch
dimension.
}
}

\strong{Note:} This package works internally only with the data format 'channels
first', i.e. all input dimensions and weight matrices must be adapted
accordingly.
}

\subsection{Implemented methods}{

An object of the Converter class can be applied to the
following methods:
\itemize{
\item Layerwise Relevance Propagation (\link{LRP}), Bach et al. (2015)
\item Deep Learning Important Feartures (\link{DeepLift}), Shrikumar et al. (2017)
\item \link{SmoothGrad}, Smilkov et al. (2017)
\item Vanilla \link{Gradient}
\item \link{ConnectionWeights} (global), Olden et al. (2004)
}
}
}
\examples{

#----------------------- Example 1: Neuralnet ------------------------------
library(neuralnet)
data(iris)

# Train a neural network
nn <- neuralnet((Species == "setosa") ~ Petal.Length + Petal.Width,
  iris,
  linear.output = FALSE,
  hidden = c(3, 2), act.fct = "tanh", rep = 1
)

# Convert the model
converter <- Converter$new(nn)

# Print all the layers
converter$model$modules_list

#----------------------- Example 2: Keras ----------------------------------
library(keras)

# Define a keras model
model <- keras_model_sequential()
model \%>\%
  layer_conv_2d(
    input_shape = c(32, 32, 3), kernel_size = 8, filters = 8,
    activation = "relu", padding = "same"
  ) \%>\%
  layer_conv_2d(
    kernel_size = 8, filters = 4,
    activation = "tanh", padding = "same"
  ) \%>\%
  layer_conv_2d(
    kernel_size = 4, filters = 2,
    activation = "relu", padding = "same"
  ) \%>\%
  layer_flatten() \%>\%
  layer_dense(units = 64, activation = "relu") \%>\%
  layer_dense(units = 1, activation = "sigmoid")

# Convert this model
converter <- Converter$new(model)

# Print the converted model as a named list
str(converter$model_dict)

#----------------------- Example 2: List  ----------------------------------

# Define a model

model <- NULL
model$input_dim <- 5
model$input_names <- list(c("Feat1", "Feat2", "Feat3", "Feat4", "Feat5"))
model$output_dim <- 2
model$output_names <- list(c("Cat", "no-Cat"))
model$layers$Layer_1 <-
  list(
    type = "Dense",
    weight = matrix(rnorm(5 * 20), 20, 5),
    bias = rnorm(20),
    activation_name = "tanh",
    dim_in = 5L,
    dim_out = 20L
  )
model$layers$Layer_2 <-
  list(
    type = "Dense",
    weight = matrix(rnorm(20 * 2), 2, 20),
    bias = rnorm(2),
    activation_name = "softmax",
    dim_in = 20L,
    dim_out = 2L
  )

# Convert the model
converter <- Converter$new(model)

# Get the model as a torch::nn_module
torch_model <- converter$model

# You can use it as a normal torch model
x <- torch::torch_randn(3, 5)
torch_model(x)


}
\references{
\itemize{
\item J. D. Olden et al. (2004) \emph{An accurate comparison of methods for
quantifying variable importance in artificial neural networks using
simulated data.} Ecological Modelling 178, p. 389â€“397
\item S. Bach et al. (2015) \emph{On pixel-wise explanations for non-linear
classifier decisions by layer-wise relevance propagation.} PLoS ONE 10,
p. 1-46
\item A. Shrikumar et al. (2017) \emph{Learning important features through
propagating activation differences.}  ICML 2017, p. 4844-4866
\item D. Smilkov et al. (2017) \emph{SmoothGrad: removing noise by adding noise.}
CoRR, abs/1706.03825
}
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{model}}{The converted neural network based on the torch module
\link{ConvertedModel}.}

\item{\code{model_dict}}{The model stored in a named list (see Details for more
information).}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{Converter$new()}}
\item \href{#method-clone}{\code{Converter$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Create a new Converter for a given neural network.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Converter$new(model, dtype = "float")}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{model}}{A trained neural network for classification or regression
tasks to be interpreted. Only models from the following types or
packages are allowed: \code{\link[keras]{keras_model}},
\code{\link[keras]{keras_model_sequential}} or
\code{\link[neuralnet]{neuralnet}}.}

\item{\code{dtype}}{The data type for the calculations. Use either \code{'float'}
or \code{'double'}}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A new instance of the R6 class \code{'Converter'}.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Converter$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
