% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LRP.R
\name{func_lrp}
\alias{func_lrp}
\title{Layer-wise Relevance Propagation (LRP) method}
\usage{
func_lrp(layers, out_class = NULL, rule_name = "simple", rule_param = NULL)
}
\arguments{
\item{layers}{List of layers of type \code{\link{Dense_Layer}}.}

\item{out_class}{If the given model is a classification model, this
parameter can be used to determine which class the relevance scores should be
calculated for. Use the default value \code{NULL} to return the relevance
for all classes.}

\item{rule_name}{The name of the rule, with which the relevance scores are
calculated. Implemented are \code{"simple"}, \code{"eps"}, \code{"ab"},
\code{"ww"} (default: \code{"simple"}).}

\item{rule_param}{The parameter of the selected rule. Note: Only the rules
\code{"eps"} and \code{"ab"} take use of the parameter. Use the default
value \code{NULL} for the default parameters ("eps" : \eqn{0.01}, "ab" : \eqn{0.5}).}
}
\value{
If \code{out_class} is \code{NULL} it returns a matrix of shape \emph{(#input features, #outputs)},
which contains the relevance scores for each input variable to the
output predictions. Otherwise it returns a vector of the relevance scores
for each input variable for the given output class.
}
\description{
This is an implementation of the \emph{Layer-wise Relevance Propagation (LRP)}
algorithm introduced by Bach et al. (2015). It's a local method for
interpreting a single element of the dataset and returns the relevance scores for
each input feature. The basic idea of this method is to decompose the
prediction score of the model with respect to the input features, i.e.
\deqn{f(x) \approx \sum_i R(x_i).}
Because of the bias vector, this decomposition is generally an approximation.
There exist several propagation rules to determine the relevance scores. In this
package are implemented: \code{\link{linear_simple_rule}},
\code{\link{linear_eps_rule}}, \code{\link{linear_ab_rule}},
\code{\link{linear_ww_rule}}.
}
\examples{
# create three dense layers
W_1 <- matrix(rnorm(3*10), nrow = 3, ncol = 10)
b_1 <- rnorm(10)
W_2 <- matrix(rnorm(10*5), nrow = 10, ncol = 5)
b_2 <- rnorm(5)
W_3 <- matrix(rnorm(5*2), nrow = 5, ncol = 2)
b_3 <- rnorm(2)

dense_1 <- Dense_Layer$new(W_1, b_1, get_activation("relu"))
dense_2 <- Dense_Layer$new(W_2, b_2, get_activation("relu"))
dense_3 <- Dense_Layer$new(W_3, b_3, get_activation("softmax"))

# do the forward pass to store all intermediate values
inputs <- rnorm(3)
output <- dense_1$forward(inputs)$out
output <- dense_2$forward(output)$out
output <- dense_3$forward(output)$out

# calculate relevances for class 1 with simple-rule
func_lrp(list(dense_1, dense_2, dense_3), out_class = 1)

# calculate relevances for all classes with simple-rule
func_lrp(list(dense_1, dense_2, dense_3))

# calculate relevances for class 1 with eps-rule (eps = 0.1)
func_lrp(list(dense_1, dense_2, dense_3), out_class = 1, rule_name = "eps", rule_param = 0.1)

}
\references{
S. Bach et al. (2015) \emph{On pixel-wise explanations for non-linear
classifier decisions by layer-wise relevance propagation.} PLoS ONE 10, p. 1-46
}
\seealso{
\code{\link{Analyzer}}, \code{\link{func_deeplift}}, \code{\link{func_connection_weights}},
\code{\link{linear_simple_rule}},
\code{\link{linear_eps_rule}}, \code{\link{linear_ab_rule}},
\code{\link{linear_ww_rule}}
}
