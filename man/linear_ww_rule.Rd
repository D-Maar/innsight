% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LRP.R
\name{linear_ww_rule}
\alias{linear_ww_rule}
\title{WÂ²-LRP-Rule for a dense layer}
\usage{
linear_ww_rule(weight, relevance)
}
\arguments{
\item{weight}{The weight matrix with size \emph{(dim_in, dim_out)} of the
hidden layer.}

\item{relevance}{A matrix with the relevance scores from each neuron of the upper
hidden layer to each output neuron of the model.}
}
\value{
Returns the relevance scores from each neuron in the lower hidden layer to each
output neuron of the model as a matrix.
}
\description{
This propagation rule is independent of the input values and takes the ratio
between the squared weight \eqn{w_{ij}} and the sum of all squared weights to
neuron \eqn{j} (\eqn{\sum_{i'} w_{i'j}^2}). It's an implementation of eq. (12) in
Montavon et al. (2005), i.e.
\deqn{R_{ij} = \frac{w_{ij}^2}{\sum_k w_{kj}^2} \cdot R_j. }
Then the relevance of the neuron \eqn{i} is given by the sum over the relevances
of all possible outgoing connections, i.e.
\deqn{R_i = \sum_j R_{ij}.}
}
\references{
G. Montavon et al. (2015) \emph{Explaining nonLinear classification decisions
with deep taylor decomposition.} CoRR
}
\seealso{
\code{\link{linear_simple_rule}}, \code{\link{linear_eps_rule}},
\code{\link{linear_ab_rule}}, \code{\link{func_lrp}}, \code{\link{Analyzer}}
}
