% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Gradients.R
\name{SmoothGrad}
\alias{SmoothGrad}
\title{SmoothGrad method}
\usage{
SmoothGrad(
  analyzer,
  data,
  n = 50,
  noise_level = 0.1,
  times_input = FALSE,
  ignore_last_act = TRUE
)
}
\arguments{
\item{analyzer}{An instance of the R6 class \code{\link{Analyzer}}.}

\item{data}{Either a matrix or a data frame, where each row must describe an
input to the network.}

\item{n}{Number of perturbations of the input vector (default: \eqn{50}).}

\item{noise_level}{Determines the standard deviation of the gaussian
perturbation, i.e. \eqn{\sigma = (max(x) - min(x)) *} \code{noise_level}.}

\item{times_input}{Multiplies the gradients with the input features. This
method is called 'Gradient x Inputs'.}

\item{ignore_last_act}{Calculate the gradients of the output or the preactivation
of the output (default: \code{TRUE}).}
}
\value{
Returns an array of size \emph{(dim_in, dim_out, num_data)} which
contains the smoothed gradients for each input variable to the
output predictions for each element in the given data.
}
\description{
SmoothGrad was introduced by D. Smilkov et al. (2017) and is an extension to
the classical \link{Gradient} method. It takes the mean of the gradients for \code{n}
perturbations of each data point, i.e. with \eqn{\epsilon ~ N(0,\sigma)}
\deqn{1/n \sum_n d f(x+ \epsilon)_j / d x_j.}
}
\references{
D. Smilkov et al. (2017) \emph{SmoothGrad: removing noise by adding noise.}
CoRR, abs/1706.03825
}
