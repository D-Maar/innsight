% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Gradients.R
\name{SmoothGrad}
\alias{SmoothGrad}
\title{SmoothGrad method}
\usage{
SmoothGrad(
  analyzer,
  data,
  n = 50,
  noise_level = 0.3,
  times_input = TRUE,
  out_class = NULL,
  ignore_last_act = TRUE
)
}
\arguments{
\item{analyzer}{An instance of the R6 class \code{\link{Analyzer}}.}

\item{data}{Either a matrix or a data frame, where each row must describe an
input to the network.}

\item{n}{Number of perturbations of the input vector (default: \eqn{50}).}

\item{noise_level}{Determines the standard deviation of the gaussian
perturbation, i.e. \eqn{\sigma = (\max(x) - \min(x)) *} \code{noise_level}.}

\item{times_input}{Multiplies the gradients with the input features. This
method is called 'Gradient x Inputs'.}

\item{out_class}{If the given model is a classification model, this
parameter can be used to determine which class the gradients should be
calculated for. Use the default value \code{NULL} to return the gradients
for all classes.}

\item{ignore_last_act}{Calculate the gradients of the output or the preactivation
of the output (default: \code{TRUE}).}
}
\value{
If \code{out_class} is \code{NULL} it returns a matrix of shape \emph{(in, out)},
which contains the gradients for each input variable to the
output predictions. Otherwise returns a vector of the gradient
for each input variable for the given output class.
}
\description{
SmoothGrad was introduced by D. Smilkov et al. (2017) and is an extension to
the classical \link{Gradient} method.
This method computes the gradients of the outputs with respect to the input
variables, i.e. for all input variable \eqn{i} and output class \eqn{j}
\deqn{\frac{\partial f(x)_j}{\partial x_i}.}
}
\references{
D. Smilkov et al. (2017) \emph{SmoothGrad: removing noise by adding noise.}
CoRR, abs/1706.03825
}
