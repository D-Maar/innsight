% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LRP.R
\name{linear_simple_rule}
\alias{linear_simple_rule}
\title{LRP: Simple-Rule for a dense layer}
\usage{
linear_simple_rule(input, weight, bias, relevance)
}
\arguments{
\item{input}{The input vector with length \emph{dim_in} of the hidden layer.}

\item{weight}{The weight matrix with size \emph{(dim_in ,dim_out)} of the
hidden layer.}

\item{bias}{The bias vector with length \emph{dim_out} of the hidden layer.}

\item{relevance}{A matrix with the relevance scores from each neuron of the upper
hidden layer to each output neuron of the model.}
}
\value{
Returns the relevance scores from each neuron in the lower hidden layer to each
output neuron of the model as a matrix.
}
\description{
This is the simplest rule for the LRP method and is an implementation of eq. (56)
in Bach et al. (2015). Let \eqn{z_{ij}:= x_i w_{ij}} the preactivation of a hidden
dense layer between hidden neuron \eqn{i} and neuron \eqn{j} in the next hidden layer.
The relevance for a single connection is propagated by the ratio between local (\eqn{z_{ij}}) and
global (\eqn{z_j := b_j + \sum_{i'} z_{i'j}}) preactivation, i.e.
\deqn{R_{ij} = \frac{z_{ij}}{z_j} \cdot R_j.}
Then the relevance of the neuron \eqn{i} is given by the sum over the relevances
of all possible outgoing connections, i.e.
\deqn{R_i = \sum_j R_{ij}.}
}
\references{
S. Bach et al. (2015) \emph{On pixel-wise explanations for non-linear
classifier decisions by layer-wise relevance propagation.} PLoS ONE 10, p. 1-46
}
\seealso{
\code{\link{linear_eps_rule}}, \code{\link{linear_ab_rule}},
\code{\link{linear_ww_rule}}, \code{\link{func_lrp}}, \code{\link{Analyzer}}
}
