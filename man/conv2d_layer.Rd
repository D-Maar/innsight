% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Layer_conv2d.R
\name{conv2d_layer}
\alias{conv2d_layer}
\title{Two-dimensional convolution layer of a Neural Network}
\usage{
conv2d_layer(
  weight,
  bias,
  dim_in,
  dim_out,
  stride = 1,
  padding = c(0, 0, 0, 0),
  dilation = 1,
  activation_name = "linear",
  dtype = "float"
)
}
\arguments{
\item{weight}{The weight matrix of dimension \emph{(out_channels, in_channels, kernel_height, kernel_width)}}

\item{bias}{The bias vector of dimension \emph{(out_channels)}}

\item{dim_in}{The input dimension of the layer: \emph{(in_channels, in_height, in_width)}}

\item{dim_out}{The output dimensions of the layer: \emph{(out_channels, out_height, oout_width)}}

\item{stride}{The stride used in the convolution, by default \code{1}}

\item{padding}{The padding of the layer, by default \code{c(0,0,0,0)} (left, right, top, bottom), can be an integer or a four-dimensional tuple}

\item{dilation}{The dilation of the layer, by default \code{1}}

\item{activation_name}{The name of the activation function used, by default \code{"linear"}}

\item{dtype}{The data type of all the parameters (Use \code{'float'} or \code{'double'})}
}
\description{
Implementation of a two-dimensional Convolutional Neural Network layer as a \code{\link[torch]{nn_conv2d}} module
where input, preactivation and output values of the last forward pass are stored
(same for a reference input, if this is needed). Applies the torch function \code{\link[torch]{nnf_conv2d}} for
forwarding an input through a 2d convolution followed by an activation function
\eqn{\sigma} to the input data, i.e.
\deqn{y= \sigma(\code{nnf_conv2d(x,W,b)})}
}
\section{Attributes}{

\describe{
\item{\code{self$W}}{The weight matrix of this layer with shape \emph{(out_channels, in_channels, kernel_height, kernel_width)}}
\item{\code{self$b}}{The bias vector of this layer with shape \emph{(out_channels)}}
\item{\code{self$...}}{Many attributes are inherited from the superclass \link{Interpreting_Layer}, e.g.
\code{input}, \code{input_dim}, \code{preactivation}, \code{activation_name}, etc.}
}
}

\section{\code{self$forward()}}{

The forward function takes an input and forwards it through the layer,
updating the the values of \code{input}, \code{preactivation} and \code{output}
\subsection{Usage}{

\code{self(x)}
}

\subsection{Arguments}{

\describe{
\item{\code{x}}{The input torch tensor of dimensions \emph{(batch_size, in_channels, in_height, in_width)}}
}
}

\subsection{Return}{

Returns the output of the layer with respect to the given inputs, with dimensions
\emph{(batch_size, out_channels, out_height, out_width)}
}
}

\section{\code{self$update_ref()}}{

This function takes the reference input and runs it through
the layer, updating the the values of \code{input_ref}, \code{preactivation_ref} and \code{output_ref}
\subsection{Usage}{

\code{self$update_ref(x_ref)}
}

\subsection{Arguments}{

\describe{
\item{\code{x_ref}}{The new reference input, of dimensions \emph{(1, in_channels, in_height, in_width)}}
}
}

\subsection{Return}{

Returns the output of the reference input after
passing through the layer, of dimension \emph{(1, out_channels, out_height, out_width)}
}
}

\section{\code{self$get_input_relevances()}}{

This method uses the output layer relevances and calculates the input layer
relevances using the specified rule.
\subsection{Usage}{

\verb{self$get_input_relevances(}\cr
\verb{  rel_output,}\cr
\verb{  rule_name = 'simple',} \cr
\verb{  rule_param = NULL)}
}

\subsection{Arguments}{

\describe{
\item{\code{rel_output}}{The output relevances, of dimensions
\emph{(batch_size, out_channels, out_height, out_width, model_out)}}
\item{\code{rule_name}}{The name of the rule, with which the relevance scores are
calculated. Implemented are \code{"simple"}, \code{"epsilon"}, \code{"alpha_beta"},
\code{"ww"} (default: \code{"simple"}).}
\item{\code{rule_param}}{The parameter of the selected rule. Note: Only the rules
\code{"epsilon"} and \code{"alpha_beta"} take use of the parameter. Use the default
value \code{NULL} for the default parameters (\code{"epsilon"} : \eqn{0.01}, \code{"alpha_beta"} : \eqn{0.5}).}
}
}

\subsection{Return}{

Returns the relevance score of the layer's input to the model output as a
torch tensor of size \emph{(batch_size, in_channels, in_height, in_width, model_out)}
}
}

\section{\code{self$get_input_multiplier()}}{

This function is the local implementation of the DeepLift method for this
layer and returns the multiplier from the input contribution to the output.
\subsection{Usage}{

\code{self$get_input_multiplier(mult_output, rule_name = "rescale")}
}

\subsection{Arguments}{

\describe{
\item{\code{mult_output}}{The multiplier of the layer output contribution
to the model output. A torch tensor of shape
\emph{(batch_size, out_channels, out_height, out_width, model_out)}}
\item{\code{rule_name}}{The name of the rule, with which the multiplier is
calculated. Implemented are \code{"rescale"} and \code{"reveal_cancel"}
(default: \code{"rescale"}).}
}
}

\subsection{Return}{

Returns the contribution multiplier of the layer's input to the model output
as torch tensor of dimension \emph{(batch_size, in_channels, in_height, in_width, model_out)}.
}
}

\section{\code{self$get_gradient()}}{

This method uses \code{\link[torch]{nnf_conv_transpose2d}} to multiply the input with the
gradient of a layer's output with respect to the layer's input. This results in the
gradients of the model output with respect to layer's input.
\subsection{Usage}{

\code{self$get_gradient(input, weight)}
}

\subsection{Arguments}{

\describe{
\item{\code{input}}{The gradients of the upper layer, a tensor of dimension
\emph{(batch_size, out_channels, out_height, out_width, model_out)}}
\item{\code{weight}}{A weight tensor of dimensions \emph{(out_channels, in_channels, kernel_height, kernel_width)}}
}
}

\subsection{Return}{

Returns the gradient of the model's output with respect to the layer input
as a torch tensor of dimension \emph{(batch_size, in_channels, in_height, in_width, model_out)}.
}
}

\section{\code{self$get_pos_and_neg_outputs()}}{

This method separates the linear layer output (i.e. the preactivation) into
the positive and negative parts.
\subsection{Usage}{

\code{self$get_pos_and_neg_outputs(input, use_bias = FALSE)}
}

\subsection{Arguments}{

\describe{
\item{\code{input}}{The input whose linear output we want to decompose into
the positive and negative parts}
\item{\code{use_bias}}{Boolean whether the bias vector should be considered
(default: FALSE)}
}
}

\subsection{Return}{

Returns a decomposition of the linear output of this layer with input \code{input}
into the positive and negative parts. A list of two torch tensors with
size \emph{(batch_size, out_channels, out_height, out_width)} and keys \verb{$pos} and \verb{$neg}
}
}

\section{\code{self$set_dtype()}}{

This function changes the data type of the weight and bias tensor to be
either \code{"float"} or \code{"double"}.
\subsection{Usage}{

\code{self$set_dtype(dtype)}
}

\subsection{Arguments}{

\describe{
\item{\code{dtype}}{The data type of the layer's parameters. Use \code{"float"} or
\code{"double"}}
}
}
}

