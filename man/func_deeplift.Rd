% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/DeepLIFT.R
\name{func_deeplift}
\alias{func_deeplift}
\title{Deep Learning Important FeaTures (DeepLIFT) method}
\usage{
func_deeplift(layers, rule_name = "rescale", out_class = NULL)
}
\arguments{
\item{layers}{List of layers of type \code{\link{Dense_Layer}}.}

\item{rule_name}{Name of the applied rule to calculate the contributions. Use one
of \code{"rescale"} and \code{"revealcancel"}.}

\item{out_class}{If the given model is a classification model, this
parameter can be used to determine which class the contribution should be
calculated for. Use the default value \code{NULL} to return the contribution
for all classes.}
}
\value{
If \code{out_class} is \code{NULL} it returns a matrix of shape \emph{(in, out)},
which contains the contribution values for each input variable to the
output predictions. Otherwise returns a vector of the contribution values
for each input variable for the given output class.
}
\description{
This is an implementation of the \emph{Deep Learning Important FeaTures (DeepLIFT)}
algorithm introduced by Shrikumar et al. (2017). It's a local method for
interpreting a single element \eqn{x} of the dataset concerning a reference value \eqn{x'}
and returns the contribution of each input feature from the difference of the
output (\eqn{y=f(x)}) and reference output (\eqn{y'=f(x')}) prediction.
The basic idea of this method is to decompose the difference-from-reference
prediction with respect to the input features, i.e.
\deqn{\Delta y = y - y'  = \sum_i C(x_i).}
Compared to \emph{Layer-wise Relevance Propagation} (see \code{\link{func_lrp}}) is the
DeepLIFT method exact and not an approximation, so we get real contributions
of the input features to the difference-from-reference prediction. There are
two ways to handle activation functions: \emph{Rescale-Rule} and \emph{Reveal-Cancel-Rule}.
}
\examples{
# create dense layers
W_1 <- matrix(rnorm(3*10), nrow = 3, ncol = 10)
b_1 <- rnorm(10)
W_2 <- matrix(rnorm(10*5), nrow = 10, ncol = 5)
b_2 <- rnorm(5)
W_3 <- matrix(rnorm(5*2), nrow = 5, ncol = 2)
b_3 <- rnorm(2)

dense_1 <- Dense_Layer$new(W_1, b_1, get_activation("relu"))
dense_2 <- Dense_Layer$new(W_2, b_2, get_activation("relu"))
dense_3 <- Dense_Layer$new(W_3, b_3, get_activation("softmax"))

# do the forward pass to store all intermediate values
# NOTE: For DeepLift a reference input is required
inputs <- rnorm(3)
inputs_ref <- inputs * c(1,0.1, 0.01)
output <- dense_1$forward(inputs, inputs_ref)
output <- dense_2$forward(output$out, output$out_ref)
output <- dense_3$forward(output$out, output$out_ref)

# print differenc-from-reference prediction before softmax activation
delta_out <- dense_3$preactivation - dense_3$preactivation_ref
delta_out

# calculate contributions for class 1
contr <- func_deeplift(list(dense_1, dense_2, dense_3), out_class = 1)
contr
sum(contr) # same as delta_out[1]

# calculate contributions for all classes
contr <- func_deeplift(list(dense_1, dense_2, dense_3))
contr
colSums(contr) # same as delta_out

# calculate contributions for class 1 with Reveal-Cancel-Rule
contr <- func_deeplift(list(dense_1, dense_2, dense_3), rule_name = "revealcancel", out_class = 1)
contr
sum(contr) # same as delta_out[1]

}
\references{
A. Shrikumar et al. (2017) \emph{Learning important features through
propagating activation differences.}  ICML 2017, p. 4844-4866
}
\seealso{
\code{\link{rescale_rule}}, \code{\link{reveal_cancel_rule}},
\code{\link{Analyzer}}, \code{\link{func_lrp}}, \code{\link{func_connection_weights}}
}
