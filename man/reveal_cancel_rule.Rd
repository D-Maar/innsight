% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/DeepLIFT.R
\name{reveal_cancel_rule}
\alias{reveal_cancel_rule}
\title{DeepLIFT: Reveal-Cancel-Rule}
\usage{
reveal_cancel_rule(delta_x, mult_x_plus, mult_x_minus, layer)
}
\arguments{
\item{delta_x}{Difference-from-reference preactivation of the current layer.}

\item{mult_x_plus}{The multiplier from the upper positive difference-from-reference
value to the output, i.e. \eqn{m_{\Delta y^+ \Delta t}}}

\item{mult_x_minus}{The multiplier from the upper negative difference-from-reference
value to the output, i.e. \eqn{m_{\Delta y^- \Delta t}}.}

\item{layer}{The hidden layer of type \code{\link{Dense_Layer}}.}
}
\value{
Returns the multiplier from the difference-from-reference preactivation to
the output, i.e. \eqn{m_{\Delta x \Delta t}}.
}
\description{
Implementation of the \emph{Reveal Cancel Rule} introduced by Shrikumar et al. (2017)
to determine a multiplier for an activation function.
This rule defines different multipliers for the negative and positive
contributions between difference-from-reference preactivation and postactivation.
The positive and negative part of the postactivation \eqn{\Delta y = f(x) - f(x')}
is given by
\deqn{\Delta y^+ := 0.5 ( f(x' + \Delta x^+) - f(x') + f(x' + \Delta x^+ + \Delta x^-) - f(x' + \Delta x^-) )}
\deqn{\Delta y^- := 0.5 ( f(x' + \Delta x^-) - f(x') + f(x' + \Delta x^+ + \Delta x^-) - f(x' + \Delta x^+) ).}
Hence the multiplier are
\deqn{m_{+ \to +} = \frac{\Delta y^+}{\Delta x^+}}
\deqn{m_{- \to -} = \frac{\Delta y^-}{\Delta x^-}.}
Afterward the contribution \eqn{m_{\Delta x \Delta t}} is calculated and returned.
}
\references{
A. Shrikumar et al. (2017) \emph{Learning important features through
propagating activation differences.}  ICML 2017, p. 4844-4866
}
\seealso{
\code{\link{rescale_rule}}, \code{\link{func_deeplift}},
\code{\link{Analyzer}}
}
