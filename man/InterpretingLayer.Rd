% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/InterpretingLayer.R
\name{InterpretingLayer}
\alias{InterpretingLayer}
\title{A Neural Network Layer for Interpreting its Input}
\usage{
InterpretingLayer()
}
\description{
Implementation of a layer of a neural network as a torch module, to be used
as a parent module to dense and convolutional layer modules. The main
difference with the pre-implemented modules in torch is that many values
are stored during the forward pass.
}
\section{Attributes}{

\describe{
\item{\code{self$input_dim}}{Dimension of the input without batch dimension}
\item{\code{self$input}}{The last recorded input for this layer}
\item{\code{self$input_ref}}{The last recorded reference input for this layer}
\item{\code{preactivation}}{The last recoreded preactivation of this layer}
\item{\code{preactivation_ref}}{The last recorded reference preactivation of
this layer}
\item{\code{self$output_dim}}{The dimension of the output of this layer}
\item{\code{self$output}}{The last recorded output of this layer}
\item{\code{self$output_ref}}{The last recored reference output of this layer}
\item{\code{activation_f}}{The activation function of this layer implemented
in torch}
\item{\code{activation_name}}{The name of the activation function}
}
}

