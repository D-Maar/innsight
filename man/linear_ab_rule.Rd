% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LRP.R
\name{linear_ab_rule}
\alias{linear_ab_rule}
\title{LRP: Alpha-Beta-Rule for a dense layer}
\usage{
linear_ab_rule(input, weight, bias, relevance, alpha = 0.5)
}
\arguments{
\item{input}{The input vector with length \emph{dim_in} of the hidden layer.}

\item{weight}{The weight matrix with size \emph{(dim_in ,dim_out)} of the
hidden layer.}

\item{bias}{The bias vector with length \emph{dim_out} of the hidden layer.}

\item{relevance}{A matrix with the relevance scores from each neuron of the upper
hidden layer to each output neuron of the model.}

\item{alpha}{Value of the factor for the positive contribution (default: \eqn{0.5}).}
}
\value{
Returns the relevance scores from each neuron in the lower hidden layer to each
output neuron of the model as a matrix.
}
\description{
This is an alternative stabilizing method compared to the LRP-rule
\code{\link{linear_simple_rule}} that does not leak relevance of treating
negative and positive preactivation separately, i.e. we have a factor
\eqn{\alpha} only for the positive part and another one \eqn{\beta = 1 - \alpha} for the
negative part of the considered ratio between local and global preactivation:
\deqn{R_{ij} = ( \alpha \frac{z_{ij}^+}{z_j^+} + \beta \frac{z_{ij}^-}{z_j^-} ) \cdot R_j.}
Then the relevance of the neuron \eqn{i} is given by the sum over the relevances
of all possible outgoing connections, i.e.
\deqn{R_i = \sum_j R_{ij}.}
}
\references{
S. Bach et al. (2015) \emph{On pixel-wise explanations for non-linear
classifier decisions by layer-wise relevance propagation.} PLoS ONE 10, p. 1-46
}
\seealso{
\code{\link{linear_simple_rule}}, \code{\link{linear_eps_rule}},
\code{\link{linear_ww_rule}}, \code{\link{LRP}}
}
