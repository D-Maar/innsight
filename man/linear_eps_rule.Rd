% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LRP.R
\name{linear_eps_rule}
\alias{linear_eps_rule}
\title{Epsilon-LRP-Rule for a dense layer}
\usage{
linear_eps_rule(input, weight, bias, relevance, eps = 0.01)
}
\arguments{
\item{input}{The input vector with length \emph{dim_in} of the hidden layer.}

\item{weight}{The weight matrix with size \emph{(dim_in ,dim_out)} of the
hidden layer.}

\item{bias}{The bias vector with length \emph{dim_out} of the hidden layer.}

\item{relevance}{A matrix with the relevance scores from each neuron of the upper
hidden layer to each output neuron of the model.}

\item{eps}{Value of the predefined stabilizer (default: \eqn{0.01}).}
}
\value{
Returns the relevance scores from each neuron in the lower hidden layer to each
output neuron of the model as a matrix.
}
\description{
This is a variant of the LRP-rule \code{\link{linear_simple_rule}} with a
predefined stabilizer \eqn{\varepsilon > 0} for the denominator and is an implementation
of eq. (58) in Bach et al. (2015). Let \eqn{z_{ij}:= x_i w_{ij}} the preactivation of a hidden
dense layer between hidden neuron \eqn{i} and neuron \eqn{j} in the next hidden layer.
The relevance for a single connection is propagated by the stabilized ratio between local (\eqn{z_{ij}}) and
global (\eqn{z_j := b_j + \sum_{i'} z_{i'j}}) preactivation, i.e.
\deqn{R_{ij} = \frac{z_{ij}}{z_j + \varepsilon\ sgn(z_j)} \cdot R_j.}
Then the relevance of the neuron \eqn{i} is given by the sum over the relevances
of all possible outgoing connections, i.e.
\deqn{R_i = \sum_j R_{ij}.}
}
\references{
S. Bach et al. (2015) \emph{On pixel-wise explanations for non-linear
classifier decisions by layer-wise relevance propagation.} PLoS ONE 10, p. 1-46
}
\seealso{
\code{\link{linear_simple_rule}}, \code{\link{linear_ab_rule}},
\code{\link{linear_ww_rule}}, \code{\link{func_lrp}}, \code{\link{Analyzer}}
}
